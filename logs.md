Train logs

- LoRA (1/4): 0.05 M trainable parameters, 124.49 M total parameters. [0.04%] 2675 MiB
- LoRA (8/32): 0.30 M trainable parameters, 124.74 M total parameters. [0.24%] 2672 MB GPU
- **LoRA (32/32)**: 1.19 M trainable parameters, 125.63 M total parameters. [0.95%] 2688 MB GPU
- LoRA (128/128): 4.73 M trainable parameters, 129.17 M total parameters. [3.66%] 2712 MB GPU

- SoftPrompt (10): 0.02 M trainable parameters, 124.46 M total parameters. [0.01%] 3021 MB GPU
- SoftPrompt (20): 0.02 M trainable parameters, 124.46 M total parameters. [0.01%] 3018 MB GPU
- SoftPrompt (100): 0.02 M trainable parameters, 124.46 M total parameters. [0.01%] 8720 MB GPU

- Train from Scratch / Full Finetune: 124.45 M trainable parameters, 124.45 M total parameters. [100.00%] 3198 MB GPU

- Pre-Trained (fit linear layer): 0.01 M trainable parameters, 124.45 M total parameters. [0.01%] 1991 MB GPU